{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports and Setting Up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "ncd_file = '/Users/dianakazarian/Desktop/ClassifierData/ncd.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_json(ncd_file, lines=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Original Data from HuffPo (NCD)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have everything we need besides the actual body of the articles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Web Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_text(url):\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        article_text = ''\n",
    "        for paragraph in soup.find_all('div', class_='primary-cli cli cli-text'):\n",
    "            article_text += paragraph.get_text() + '\\n'\n",
    "        return article_text.strip()\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while scraping:\", e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "executor = ThreadPoolExecutor(max_workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_single_url(row):\n",
    "    url = row.link\n",
    "    try:\n",
    "        article_text = scrape_article_text(url)\n",
    "        return {'link': url, 'article_text': article_text}\n",
    "    except Exception as e:\n",
    "        print(\"Error occurred while scraping:\", e)\n",
    "        return {'link': url, 'article_text': None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "subset_data = data.iloc[:11842]\n",
    "subset_data['article_text'] = subset_data['link'].apply(scrape_article_text)\n",
    "\n",
    "results = executor.map(scrape_single_url, subset_data.itertuples())\n",
    "\n",
    "# Convert the results to a list of dictionaries and create new df\n",
    "results_list = list(results)\n",
    "result_df = pd.DataFrame(results_list)\n",
    "\n",
    "# Merge result with original\n",
    "df = pd.merge(subset_data, result_df, on='link', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = executor.map(scrape_single_url, data.itertuples())\n",
    "results_list = list(results)\n",
    "result_df = pd.DataFrame(results_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[['link', 'headline', 'category', 'short_description', 'authors', 'date', 'article_text_x']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Sample Article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['article_text_x'][1024]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[(df['article_text_x'] != '') & (df['article_text_x'].notna())]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New dataframe with `article_text` column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "classification_df['class'] = df['category'].map(lambda x: 1 if x == 'POLITICS' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_df.to_csv('/Users/dianakazarian/Desktop/ClassifierData/classifierdata.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "ebb543f52eee5d43215072dfd434f012f2cdae7c522a0728331ba0d913637a94"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
